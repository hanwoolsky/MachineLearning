{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10_EnsembleLearning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNSbfOMToQaWhRrjKktSw09"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yzAuMp64QUfo"},"source":["#정형 vs 비정형\n","---\n","구조화 된 데이터들은 데이터베이스나 엑셀에 표현하기 용이하다. 이러한 데이터를 정형 데이터(structured data)라 한다. 반대로 텍스트 데이터나 사진처럼 엑셀로 표현하기 어려운 데이터를 비정형 데이터(unstructured date)라 한다. 정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘이 앙상블 학습이다. 이 알고리즘은 대부분 결정 트리를 기반으로 만들어져 있다. 반대로 비정형 데이터에는 신경망 알고리즘을 사용한다."]},{"cell_type":"markdown","metadata":{"id":"TWLpv9xnRSLW"},"source":["#랜덤 포레스트(Random Forest)\n","---\n","랜덤 포레스트는 앙상블 학습의 대표 주자 중 하나로안정적인 성능 덕분에 널리 사용되고 있다. 랜덤 포레스트는 결정 트리를 랜덤하게 만들어 결정 트리(나무)의 숲을 만든 후 각 결정 트리의 예측을 사용해 최종 예측을 만든다. 랜덤 포르스트는 각 트리를 훈련하기 위해 랜덤한 데이터를 만드는데, 훈련 데이터에서 중복을 허용하여 샘플을 추출한다. 이렇게 만들어진 샘플을 **부트스트랩 샘플(bootstrap sample)**이라고 한다. 기본적으로 부트스트랩 샘플은 훈련 세트의 크기와 같다. 각 노드를 분할할 때, RandomForestClassifier 분류 모델은 일반적으로 전체 특성 개수의 제곱근만큼의 특성을 무작위로 선택하여 이 중에서 최선의 분할을 찾는다. 반면 RandomForestRegressor 회귀 모델은 전체 특성을 사용한다.  "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SbZap5i2PdtQ","executionInfo":{"status":"ok","timestamp":1624593627236,"user_tz":-540,"elapsed":2588,"user":{"displayName":"성균관대허한울","photoUrl":"","userId":"02142965350981481937"}},"outputId":"f864e0cf-658f-4acf-b3eb-812127c7cee2"},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_validate\n","from sklearn.ensemble import RandomForestClassifier\n","\n","wine = pd.read_csv('https://bit.ly/wine_csv_data') # 데이터 불러온 후 세트 나누기\n","data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n","target = wine['class'].to_numpy()\n","train_input, test_input, train_target, test_target = train_test_split(data, target, test_size = 0.2, random_state = 42)\n","\n","rf = RandomForestClassifier(n_jobs = -1, random_state = 42) # 교차 검증 수행\n","scores = cross_validate(rf, train_input, train_target, return_train_score = True, n_jobs = -1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score'])) # 과대적합"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9973541965122431 0.8905151032797809\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WTTtjTmNU3uX","executionInfo":{"status":"ok","timestamp":1624593711734,"user_tz":-540,"elapsed":642,"user":{"displayName":"성균관대허한울","photoUrl":"","userId":"02142965350981481937"}},"outputId":"1c4625cb-8160-4ddb-ab8b-d4b2c5f2fb54"},"source":["rf.fit(train_input, train_target)\n","print(rf.feature_importances_) # 특성의 일부를 랜덤하게 선택하여 훈련하기 때문에 하나의 특성에 과도하게 집중하지 않는다."],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.23167441 0.50039841 0.26792718]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5M8z41-xVQG-"},"source":["랜덤 포레스트는 훈련 세트에서 중복을 허용하여 샘플을 만들기 때문에 남는 샘플이 존재한다. 이런 샘플을 **OOB(out of bag)** 샘플이라고 한다. 이 남는 샘플이 검증 세트의 역할을 한다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eO_txgSqVMmh","executionInfo":{"status":"ok","timestamp":1624593878444,"user_tz":-540,"elapsed":678,"user":{"displayName":"성균관대허한울","photoUrl":"","userId":"02142965350981481937"}},"outputId":"51de6409-5572-4154-d6f6-9ab3185ff7f4"},"source":["rf = RandomForestClassifier(oob_score = True, n_jobs = -1, random_state = 42)\n","rf.fit(train_input, train_target)\n","print(rf.oob_score_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.8934000384837406\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dW3rTvlwV3TZ"},"source":["#엑스트라 트리(Extra Tree)\n","---\n","엑스트라 트리는 랜덤 포레스트와 매우 비슷하게 동작한다. 차이점은, 엑스트라 트리는 부트스트랩 샘플을 사용하지 않는다는 것이다. 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할한다. 하나의 결정 트리에서 특성을 무작위로 분할한다면 성능이 낮아지겠지만, 많은 트리를 앙상블 하기 때문에 과대적합을 막고 검증 세트의 점수를 높이는 효과가 있다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gp45RbasWqkb","executionInfo":{"status":"ok","timestamp":1624594209796,"user_tz":-540,"elapsed":3026,"user":{"displayName":"성균관대허한울","photoUrl":"","userId":"02142965350981481937"}},"outputId":"00001940-a85a-4c86-bd33-9228e62beb2c"},"source":["from sklearn.ensemble import ExtraTreesClassifier\n","\n","et = ExtraTreesClassifier(n_jobs = -1, random_state = 42)\n","scores = cross_validate(et, train_input, train_target, return_train_score = True, n_jobs = -1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9974503966084433 0.8887848893166506\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aDBzTXqTXMbG"},"source":["#그래이디언트 부스팅(Gradient Boosting)\n","---\n","그래이디언트 부스킹은 깊이가 얕은 결정 트리를 사용하여 이진 트리의 오차를 보완하는 방식으로 앙상블 하는 방법이다. 경사 하강법을 사용하여 트리를 앙상블에 추가한다. 분류에서는 로지스틱 손실 함수를, 회귀에서는 평균 제곱 오차 함수를 사용한다. 경사 하강법에서 손실 함수의 낮은 곳으로 조금씩 이동하듯이, 그래이디언트 부스팅도 결정 트리를 계속 추가하면서 가장 낮은 곳을 찾아 이동하기 때문에 깊이가 얕은 트리를 사용한다. 사이킷런의 GradientBoostingClassifier는 기본적으로 깊이가 3인 결정 트리 100개를 사용한다. 깊이가 얕기 떄문에 과대적합에 강하고 높은 일반화 성능을 기대할 수 있다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vLGM1zkcYO7E","executionInfo":{"status":"ok","timestamp":1624594603212,"user_tz":-540,"elapsed":2427,"user":{"displayName":"성균관대허한울","photoUrl":"","userId":"02142965350981481937"}},"outputId":"d3ff682c-f166-4c02-a681-31dde4a2ff1f"},"source":["from sklearn.ensemble import GradientBoostingClassifier\n","\n","gb = GradientBoostingClassifier(random_state = 42)\n","scores = cross_validate(gb, train_input, train_target, return_train_score = True, n_jobs = -1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score'])) # 거의 과대적합 되지 않는다."],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.8881086892152563 0.8720430147331015\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NhiW2zuGYt51","executionInfo":{"status":"ok","timestamp":1624594737322,"user_tz":-540,"elapsed":6554,"user":{"displayName":"성균관대허한울","photoUrl":"","userId":"02142965350981481937"}},"outputId":"b8f6d742-f5a5-4e53-8648-2dec1e38b8f0"},"source":["gb = GradientBoostingClassifier(n_estimators = 500, learning_rate = 0.2, random_state = 42) # 학습률을 증가시키고 트리의 개수를 늘려 성능 향상\n","scores = cross_validate(gb, train_input, train_target, return_train_score = True, n_jobs = -1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9464595437171814 0.8780082549788999\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TTafaPTxZKnH"},"source":["#히스토그램 기반 GB(Histogram-based GBoosting)\n","---\n","히스토그램 기반 그래이디언트 부스팅은 정형 데이터를 다루는 머신러닝 알고리즘 중에 가장 인기가 높은 알고리즘이다. 이 알고리즘은 입력 특성을 256개 구간으로 나누기 때문에 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"luSsv8kuaF6X","executionInfo":{"status":"ok","timestamp":1624595127010,"user_tz":-540,"elapsed":2882,"user":{"displayName":"성균관대허한울","photoUrl":"","userId":"02142965350981481937"}},"outputId":"46725552-1341-421f-eec6-a0270422bede"},"source":["from sklearn.experimental import enable_hist_gradient_boosting\n","from sklearn.ensemble import HistGradientBoostingClassifier\n","\n","hgb = HistGradientBoostingClassifier(random_state = 42)\n","scores = cross_validate(hgb, train_input, train_target, return_train_score = True, n_jobs = -1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9321723946453317 0.8801241948619236\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ooTf8U2-au1t","executionInfo":{"status":"ok","timestamp":1624595413285,"user_tz":-540,"elapsed":2063,"user":{"displayName":"성균관대허한울","photoUrl":"","userId":"02142965350981481937"}},"outputId":"edcaf19d-25ac-4bee-ef5e-87e62fbcf07f"},"source":["from sklearn.inspection import permutation_importance\n","\n","hgb.fit(train_input, train_target)\n","train_result = permutation_importance(hgb, train_input, train_target, n_repeats = 10, random_state = 42, n_jobs = -1)\n","test_result = permutation_importance(hgb, test_input, test_target, n_repeats = 10, random_state = 42, n_jobs = -1)\n","print('훈련 세트의 특성 중요도:', train_result.importances_mean, '테스트 세트의 특성 중요도:', test_result.importances_mean)\n","hgb.score(test_input, test_target)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["훈련 세트의 특성 중요도: [0.08876275 0.23438522 0.08027708] 테스트 세트의 특성 중요도: [0.05969231 0.20238462 0.049     ]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.8723076923076923"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1NA1uN--b0wg","executionInfo":{"status":"ok","timestamp":1624595568930,"user_tz":-540,"elapsed":644,"user":{"displayName":"성균관대허한울","photoUrl":"","userId":"02142965350981481937"}},"outputId":"f5a0b9c9-097c-45d2-a73f-4767ec9b286a"},"source":["from xgboost import XGBClassifier # 사이킷런 말고도 히스토_그래디언트를 구현한 XGBoost 라이브러리\n","\n","xgb = XGBClassifier(tree_method = 'hist', random_state = 42)\n","scores = cross_validate(xgb, train_input, train_target, return_train_score = True)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.8824322471423747 0.8726214185237284\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nEhbp9hFcYYl","executionInfo":{"status":"ok","timestamp":1624595708667,"user_tz":-540,"elapsed":1494,"user":{"displayName":"성균관대허한울","photoUrl":"","userId":"02142965350981481937"}},"outputId":"ba18a986-3709-4053-fc00-76ff7dead720"},"source":["from lightgbm import LGBMClassifier # 사이킷런 말고도 히스토_그래디언트를 구현한 LightGBM 라이브러리\n","\n","lgb = LGBMClassifier(tree_method = 'hist', random_state = 42)\n","scores = cross_validate(lgb, train_input, train_target, return_train_score = True, n_jobs = -1)\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9338079582727165 0.8789710890649293\n"],"name":"stdout"}]}]}